# Arabic Dialect Identification under Scrutiny

[![Huggingface Space](https://img.shields.io/badge/ðŸ¤—-Demo%20-yellow.svg)](https://huggingface.co/AMR-KELEG/ADI-NADI-2023)
[![Data](https://img.shields.io/badge/Error_Analysis-Annotations-blue)](https://github.com/AMR-KELEG/ADI-under-scrutiny/raw/master/data/annotations.tar.gz)
[![arXiv](https://img.shields.io/badge/arXiv-2310.13661-00ff00.svg)](https://arxiv.org/abs/2310.13661)

## Experiment #1 - Estimating the Maximal Accuracy of Parallel Corpora
1. Create a directory for the data:
```
mkdir -p data/
```

2. Download the datasets to the `data/` directory:
- Multidialectal Parallel Corpus of Arabic: https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/computational-approaches-to-modeling-language-lab/resources.html
- PADIC: https://sourceforge.net/projects/padic/
- Multi-Arabic Dialect Corpus (MADAR Corpus) - (Dialect Identification Shared Task Dataset (WANLP 2019) - Dataset and Code for MADAR Shared Task): https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/computational-approaches-to-modeling-language-lab/resources.html

3. Extract the files in the `data/` directory. The expected structure is:
```
data
â”œâ”€â”€ MADAR-SHARED-TASK-final-release-25Jul2019
â”‚   â”œâ”€â”€ MADAR-DID-Scorer.py
â”‚   â”œâ”€â”€ MADAR-Shared-Task-Subtask-1
â”‚   â”‚   â”œâ”€â”€ EXAMPLE.GOLD
â”‚   â”‚   â”œâ”€â”€ EXAMPLE.PRED
â”‚   â”‚   â”œâ”€â”€ MADAR-Corpus-26-dev.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Corpus-26-test.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Corpus-26-train.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Corpus-6-dev.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Corpus-6-train.tsv
â”‚   â”‚   â””â”€â”€ MADAR-Corpus-Lexicon-License.txt
â”‚   â”œâ”€â”€ MADAR-Shared-Task-Subtask-2
â”‚   â”‚   â”œâ”€â”€ EXAMPLE.GOLD
â”‚   â”‚   â”œâ”€â”€ EXAMPLE.PRED
â”‚   â”‚   â”œâ”€â”€ MADAR-Obtain-Tweets.py
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Corpus-License.txt
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Subtask-2.DEV.user-label.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Subtask-2.DEV.user-tweets-features.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Subtask-2.TEST.user-label.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Subtask-2.TEST.user-tweets-features.tsv
â”‚   â”‚   â”œâ”€â”€ MADAR-Twitter-Subtask-2.TRAIN.user-label.tsv
â”‚   â”‚   â””â”€â”€ MADAR-Twitter-Subtask-2.TRAIN.user-tweets-features.tsv
â”‚   â”œâ”€â”€ MADAR_SharedTask_Summary_Paper_WANLP_ACL_2019.pdf
â”‚   â””â”€â”€ README.txt
â”œâ”€â”€ MADAR-SHARED-TASK-final-release-25Jul2019.zip
â”œâ”€â”€ MultiDial-Public-Version-2014
â”‚   â”œâ”€â”€ LICENSE.txt
â”‚   â”œâ”€â”€ Multidialectal_Parallel_Corpus_of_Arabic
â”‚   â”‚   â”œâ”€â”€ EG.txt
â”‚   â”‚   â”œâ”€â”€ EN.txt
â”‚   â”‚   â”œâ”€â”€ JO.txt
â”‚   â”‚   â”œâ”€â”€ l.txt
â”‚   â”‚   â”œâ”€â”€ MSA.txt
â”‚   â”‚   â”œâ”€â”€ PA.txt
â”‚   â”‚   â”œâ”€â”€ SY.txt
â”‚   â”‚   â””â”€â”€ TN.txt
â”‚   â”œâ”€â”€ Multidialectal_Parallel_Corpus_of Arabic_Paper.pdf
â”‚   â””â”€â”€ README.txt
â”œâ”€â”€ MultiDial-Public-Version-2014.zip
â””â”€â”€ PADIC.xml
```

4. Run the Expected Maximal Accuracy estimation script:
```
python estimate_maximal_accuracy_parallel_corpora.py
```
**Note**: The script prints the metrics and saves intermediate data files to `data/preprocessed`.

## Experiment #2 - Analyzing the errors of a MarBERT model
1. Create the data directory
```
mkdir -p data/
```

2. Download the datasets to the `data/` directory
- NADI 2023
- QADI: http://alt.qcri.org/resources/qadi/

**Note**: QADI's zip is password protected. The password can be found through the dataset's page.

3. Extract the zip files:
```
cd data && unzip NADI2023_Release_Train.zip && cd ..
cd data && unzip QADI_Corpus_r001.zip && cd ..
```

The expected structure is:
```
data
â”œâ”€â”€ NADI2023_Release_Train
â”‚   â”œâ”€â”€ NADI2023-README.txt
â”‚   â”œâ”€â”€ NADI2023-Twitter-Corpus-License.txt
â”‚   â”œâ”€â”€ Subtask1
â”‚   â”‚   â”œâ”€â”€ MADAR-2018.tsv
â”‚   â”‚   â”œâ”€â”€ NADI2020-TWT.tsv
â”‚   â”‚   â”œâ”€â”€ NADI2021-TWT.tsv
â”‚   â”‚   â”œâ”€â”€ NADI2023-ST1-Scorer.py
â”‚   â”‚   â”œâ”€â”€ NADI2023_Subtask1_DEV.tsv
â”‚   â”‚   â”œâ”€â”€ NADI2023_Subtask1_TRAIN.tsv
â”‚   â”‚   â”œâ”€â”€ subtask1_dev_GOLD.txt
â”‚   â”‚   â””â”€â”€ ubc_subtask1_dev_1.txt.zip
â”‚   â”œâ”€â”€ Subtask2
â”‚   â”‚   â”œâ”€â”€ NADI2023-ST2-Scorer.py
â”‚   â”‚   â”œâ”€â”€ subtask2_dev_GOLD.txt
â”‚   â”‚   â”œâ”€â”€ subtask2_dev.tsv
â”‚   â”‚   â””â”€â”€ ubc_subtask2_dev_1.txt.zip
â”‚   â””â”€â”€ Subtask3
â”‚       â”œâ”€â”€ NADI2023-ST3-Scorer.py
â”‚       â”œâ”€â”€ subtask3_dev_GOLD.txt
â”‚       â”œâ”€â”€ subtask3_dev.tsv
â”‚       â””â”€â”€ ubc_subtask3_dev_1.txt.zip
â”œâ”€â”€ NADI2023_Release_Train.zip
â”œâ”€â”€ QADI_Corpus
â”‚   â”œâ”€â”€ dataset
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_AE.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_BH.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_DZ.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_EG.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_IQ.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_JO.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_KW.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_LB.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_LY.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_MA.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_OM.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_PL.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_QA.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_SA.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_SD.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_SY.txt
â”‚   â”‚   â”œâ”€â”€ QADI_train_ids_TN.txt
â”‚   â”‚   â””â”€â”€ QADI_train_ids_YE.txt
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ testset
â”‚       â””â”€â”€ QADI_test.txt
â””â”€â”€ QADI_Corpus_r001.zip
```

4. Unify the NADI datasets labels
```
python preprocess_NADI_QADI.py
```

5. Fine-tune MarBERT using NADI2023's training data
```
OUTPUT_DIR="models/basic_NADI2023"
mkdir -p "${OUTPUT_DIR}"

CUDA_VISIBLE_DEVICES="0" python finetune-BERT.py train -label_smoothing_factor 0 -d data/preprocessed/NADI2023_train.tsv --dev data/preprocessed/NADI2023_dev.tsv -o "${OUTPUT_DIR}" 2>&1 | tee ~/outputfile.txt "${OUTPUT_DIR}/training_logs.txt"
```

6. Generate the predictions for NADI2021's training dataset and QADI's test dataset
```
MODEL_DIR="models/basic_NADI2023"

OUTPUT_DIR="${MODEL_DIR}/predictions/"
mkdir -p "${OUTPUT_DIR}"

CUDA_VISIBLE_DEVICES="0" python finetune-BERT.py predict -d data/preprocessed/NADI2021_DA_train.tsv -p "${MODEL_DIR}/checkpoint-1689" -o "${OUTPUT_DIR}"/NADI2021_DA_train_pred.tsv
CUDA_VISIBLE_DEVICES="0" python finetune-BERT.py predict -d data/preprocessed/QADI.tsv -p "${MODEL_DIR}/checkpoint-1689" -o "${OUTPUT_DIR}"/QADI_pred.tsv
```

7. Generate the error files from QADI
```
python analyze_QADI.py
```

## Citation
```
@inproceedings{keleg-magdy-2023-arabic,
    title = "{A}rabic Dialect Identification under Scrutiny: Limitations of Single-label Classification",
    author = "Keleg, Amr  and
      Magdy, Walid",
    editor = "Sawaf, Hassan  and
      El-Beltagy, Samhaa  and
      Zaghouani, Wajdi  and
      Magdy, Walid  and
      Abdelali, Ahmed  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Habash, Nizar  and
      Khalifa, Salam  and
      Keleg, Amr  and
      Haddad, Hatem  and
      Zitouni, Imed  and
      Mrini, Khalil  and
      Almatham, Rawan",
    booktitle = "Proceedings of ArabicNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.arabicnlp-1.31",
    doi = "10.18653/v1/2023.arabicnlp-1.31",
    pages = "385--398",
    abstract = "Automatic Arabic Dialect Identification (ADI) of text has gained great popularity since it was introduced in the early 2010s. Multiple datasets were developed, and yearly shared tasks have been running since 2018. However, ADI systems are reported to fail in distinguishing between the micro-dialects of Arabic. We argue that the currently adopted framing of the ADI task as a single-label classification problem is one of the main reasons for that. We highlight the limitation of the incompleteness of the Dialect labels and demonstrate how it impacts the evaluation of ADI systems. A manual error analysis for the predictions of an ADI, performed by 7 native speakers of different Arabic dialects, revealed that $\approx$ 67{\%} of the validated errors are not true errors. Consequently, we propose framing ADI as a multi-label classification task and give recommendations for designing new ADI datasets.",
}
```
